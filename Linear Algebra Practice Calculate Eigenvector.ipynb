{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/5/58/Eigenvalue_equation.svg/250px-Eigenvalue_equation.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "http://math.colgate.edu/~wweckesser/math312Spring06/handouts/IMM_2x2linalg.pdf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Casting neural networks as matrices and vectors enables one to utilize many of the properties of matrices and vectors to compactly handle computations.  Matrices and vectors are a very convenient way of representing neural networks.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2], [3,-4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2],\n",
       "       [ 3, -4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Below is the Identity matrix.  Identity matrix is equivalent to multiply a matrix by a scaler of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.identity(A.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eigenvalues, Eigenvectors = LA.eig(np.array([[1, 2], [3, -4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2., -5.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.],\n",
       "       [ 3., -4.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(A * Eigenvectors [:,1])/Eigenvectors[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.89442719, -0.31622777],\n",
       "       [ 0.4472136 ,  0.9486833 ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eigenvectors"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The results that we cacluate manually (shown below) are [2, -1] [1, 3].  The python numpy import linear algebra is normalized the array to these eigenvectors.  [2, 1] are being scaled by 2.236  and [-1, 3] are being scaled by 3.162."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20190128_162149first.jpg![](20190128_162149first.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](20190128_163200middle.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](20190128_163556last.jpg)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Matrices and vectors play an essential role in neural networks.  Vectors and matrices help by compressing all the calculations into very simple notations and their operations are fast and easy, especially when training on GPUs. \n",
    "Matrix multiplication is one of the most important operations in deep learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
