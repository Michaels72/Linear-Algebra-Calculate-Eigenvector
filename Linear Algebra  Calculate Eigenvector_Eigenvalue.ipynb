{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*'Since neural networks can only train on vectors, vectorizing data is a necessary pre-processing step.'*\n",
    "\n",
    "\"Deep Learning: A Practitioner's Approach\" by Adam Gibson and Josh Patterson (O'Reily).\n",
    "\n",
    "Casting neural networks as matrices and vectors enables one to utilize many of the properties of matrices and vectors to compactly handle computations.  Matrices and vectors are a very convenient way of representing neural networks.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,2], [3,-4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2],\n",
       "       [ 3, -4]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Below is the Identity matrix.  Identity matrix is equivalent to multiply a matrix by a scaler of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.identity(A.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eigenvalues, Eigenvectors = LA.eig(np.array([[1, 2], [3, -4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2., -5.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.89442719, -0.31622777],\n",
       "       [ 0.4472136 ,  0.9486833 ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eigenvectors"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The results that we cacluate manually (shown below) are [2, -1] [1, 3].  The python numpy import linear algebra is scaling the array to these eigenvectors.  [2, 1] are being scaled by 2.236  and [-1, 3] are being scaled by 3.162.  Python is normalizing to allow for more efficient calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20190128_162149first.jpg![](20190128_162149first.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](20190128_163200middle.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](20190128_163556last.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclustion:** Matrices and vectors play an essential role in neural networks.  Vectors and matrices help by compressing all the calculations into very simple notations and their operations are fast and easy, especially when training on GPUs.  Matrix multiplication is one of the most important operations in deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green, font size = 11 >**Appendix**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a GPU?**  GPU (Graphics processing unit) - a specialized electronic circuit designed to rapidly manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Great learning resource for linear algebra:**\n",
    "    \n",
    "https://www.youtube.com/results?search_query=3blue1brown+linear+algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a matrix?**\n",
    "\n",
    "A matrix is an m × n array of scalars from a given field.  The individual values in the matrix are called entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example:](Screenshot from 2019-01-29 09-43-51.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a vector?** \n",
    "\n",
    "Vectors are lists of numbers. In a geometric sense, a vector is where each number represents coordinates relative to some axes. Vectors are also described as just coordinates on the plane.  For example, [1,1] is a vector pointing up and to the right from the origin to the coordinate (1,1).  Vectors can also be seen as geometric transformations of the plane such as rotating, squashing, shearing or stretching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is an eigenvector?**  An eigenvector of a linear transformation is a non-zero vector that changes by only a scalar factor when that linear transformation is applied to it. \n",
    "\n",
    "**What is an eigenvalue?**  An eigenvalue is a set of values of a parameter for which a differential equation has a nonzero solution (an eigenfunction) under given conditions.  An eigenvalue is also any number such that a given matrix minus that number times the identity matrix has zero determinant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Screenshot from 2019-01-29 09-59-28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/5/58/Eigenvalue_equation.svg/250px-\n",
    "Eigenvalue_equation.svg.png)\n",
    "\n",
    "Matrix A acts by stretching the vector x, not changing its direction, so x is an eigenvector of A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eigenvalues of geometric transformations** - The following table presents some example transformations in the plane along with their 2×2 matrices, eigenvalues, and eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](Screenshot from 2019-01-29 10-04-33.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Homework practice**\n",
    "\n",
    "http://math.colgate.edu/~wweckesser/math312Spring06/handouts/IMM_2x2linalg.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
